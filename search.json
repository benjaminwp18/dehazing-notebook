[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dehazing Notebook",
    "section": "",
    "text": "Preface\nNotebook on dehazing progress.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "1  Bibliography",
    "section": "",
    "text": "Dehazing is a wide class of image processing problems, with several categories of solution:\n\nModel-based dehazing uses a model of light propagation into the camera sensor (an Image Formation Model, see [1] and [2]) in conjunction with a statistical prior about the distribution of intensities in images without haze. The success of model-based methods depends significantly on the quality of their priors, which are therefore designed to apply to as restricted a class of images as possible.\nModel-free dehazing uses more traditional image processing techniques to improve the quality of hazy images (histogram equalization, color correction, etc.).\nMachine learning solutions usually run CNNs on hazy images to predict the ground truth haze-free scene.\nFinally, some systems use specialized hardware like polarizing filters, lasers, or multiple camera sensors to gain extra information about a scene before dehazing. I will not consider these.\n\nThus far, I believe these algorithms represent good starting points to test dehazing performance:\n\n\n\nAlgorithm\nYear\nTechnique\nFluid\nMedia\n\n\n\n\nDCP [3]\n2009\nModel-based\nAir\nImage\n\n\nUDCP [4]\n2013\nModel-based\nWater\nImage\n\n\nMIP [5]\n2010\nModel-based\nWater\nImage\n\n\nHL [6]\n2016\nModel-based\nAir\nImage\n\n\nHL (underwater) [7]\n2021\nModel-based\nWater\nImage\n\n\nULAP [8]\n2018\nModel-based\nWater\nImage\n\n\nDCP + TF [9]\n2016\nModel-based\nAir\nVideo\n\n\nUWCNN [10]\n2020\nLearning\nWater\nVideo\n\n\nWaterNet [11]\n2020\nLearning\nWater\nImage\n\n\nUIEC^2-Net [12]\n2021\nLearning\nWater\nImage\n\n\n\nSome of these models were selected for their high performance and recent publication, but some were selected because they represent baseline algorithms against which to compare other models. I still need to identify pipelines that fit these criteria in the model-free domain, and to find more recent algorithms which target video dehazing.\n\n\n\n\n[1] J. S. Jaffe, “Computer modeling and the design of optimal underwater imaging systems,” IEEE Journal of Oceanic Engineering, vol. 15, no. 2, pp. 101–111, Apr. 1990, doi: 10.1109/48.50695.\n\n\n[2] Y. Y. Schechner and N. Karpel, “Clear underwater vision,” in Proceedings of the 2004 IEEE computer society conference on computer vision and pattern recognition, 2004. CVPR 2004., Jun. 2004, pp. I–I. doi: 10.1109/CVPR.2004.1315078.\n\n\n[3] K. He, J. Sun, and X. Tang, “Single image haze removal using dark channel prior,” in 2009 IEEE conference on computer vision and pattern recognition, Jun. 2009, pp. 1956–1963. doi: 10.1109/CVPR.2009.5206515.\n\n\n[4] P. Drews, E. R. Nascimento, F. Moraes, S. S. C. Botelho, and M. F. Montenegro Campos, “Transmission estimation in underwater single images,” in 2013 IEEE international conference on computer vision workshops, Dec. 2013, pp. 825–830. doi: 10.1109/ICCVW.2013.113.\n\n\n[5] N. Carlevaris-Bianco, A. Mohan, and R. M. Eustice, “Initial results in underwater single image dehazing,” in OCEANS 2010 MTS/IEEE SEATTLE, Sep. 2010, pp. 1–8. doi: 10.1109/OCEANS.2010.5664428.\n\n\n[6] D. Berman, T. Treibitz, and S. Avidan, “Non-local image dehazing,” in 2016 IEEE conference on computer vision and pattern recognition (CVPR), Jun. 2016, pp. 1674–1682. doi: 10.1109/CVPR.2016.185.\n\n\n[7] D. Berman, D. Levy, S. Avidan, and T. Treibitz, “Underwater single image color restoration using haze-lines and a new quantitative dataset,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 8, pp. 2822–2837, Aug. 2021, doi: 10.1109/TPAMI.2020.2977624.\n\n\n[8] W. Song, Y. Wang, D. Huang, and D. Tjondronegoro, “A rapid scene depth estimation model based on underwater light attenuation prior for underwater image restoration,” in Advances in multimedia information processing – PCM 2018, R. Hong, W.-H. Cheng, T. Yamasaki, M. Wang, and C.-W. Ngo, Eds., Cham: Springer International Publishing, 2018, pp. 678–688.\n\n\n[9] C. Qing, F. Yu, X. Xu, W. Huang, and J. Jin, “Underwater video dehazing based on spatial–temporal information fusion,” Multidim Syst Sign Process, vol. 27, no. 4, pp. 909–924, Oct. 2016, doi: 10.1007/s11045-016-0407-2.\n\n\n[10] C. Li, S. Anwar, and F. Porikli, “Underwater scene prior inspired deep underwater image and video enhancement,” Pattern Recognition, vol. 98, p. 107038, Feb. 2020, doi: 10.1016/j.patcog.2019.107038.\n\n\n[11] C. Li et al., “An underwater image enhancement benchmark dataset and beyond,” IEEE Transactions on Image Processing, vol. 29, pp. 4376–4389, 2020, doi: 10.1109/TIP.2019.2955241.\n\n\n[12] Y. Wang, J. Guo, H. Gao, and H. Yue, “UIEC^2-net: CNN-based underwater image enhancement using two color space,” Signal Processing: Image Communication, vol. 96, p. 116250, Aug. 2021, doi: 10.1016/j.image.2021.116250.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bibliography</span>"
    ]
  },
  {
    "objectID": "2025.06.27.html",
    "href": "2025.06.27.html",
    "title": "2  Basic metrics",
    "section": "",
    "text": "I evaluated output quality on the DeepBlue reference dataset for TCLAHE, DCP, UDCP, and TCLAHE followed by UDCP. I used reference metrics MSE, PSNR, and SSIM, and referenceless metrics entropy and UCIQE. See Figure 2.1.\n\n\nCode\nimport json\n\nPRETTY_METRIC_NAMES = {\n  'mse': 'MSE (↓)',\n  'psnr': 'PSNR (↑)',\n  'ssim': 'SSIM (↑)',\n  'entropy': 'Entropy (↑)',\n  'uciqe': 'UCIQE (↑)'\n}\nREF_METRICS = ('mse', 'psnr', 'ssim')\nREFLESS_METRICS = ('entropy', 'uciqe')\nPIPELINES = ('hazy', 'dcp', 'udcp', 'tclahe', 'tclahe_udcp')\nPIPELINE_COLORS = {\n  'dcp': 'green',\n  'udcp': 'blue',\n  'tclahe': 'red',\n  'tclahe_udcp': 'purple',\n  'hazy': 'black'\n}\nFILENAMES = ('00gt.png', '01.png', '02.png', '03.png', '04.png', '05.png', '06.png', '07.png', '08.png', '09.png', '10.png', '11.png', '12.png', '13.png', '14.png', '15.png', '16.png', '17.png', '18.png', '19.png', '20.png')\nFILENAMES_NO_EXTENSION = [filename[:-4] for filename in FILENAMES]\n\nref_data = {}\nrefless_data = {}\n\ndef make_metric_data(metrics_json, metric_name: str):\n  data = []\n  for pipeline in PIPELINES:\n    data.append([])\n    for filename in FILENAMES:\n      data[-1].append(metrics_json[pipeline][filename][metric])\n\n  return data\n\nwith open('../data/metrics.json', 'r') as metrics:\n  metrics_json = json.load(metrics)\n  for metric in REF_METRICS:\n    ref_data[metric] = make_metric_data(metrics_json, metric)\n  for metric in REFLESS_METRICS:\n    refless_data[metric] = make_metric_data(metrics_json, metric)\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nfor metrics, data in ((REF_METRICS, ref_data), (REFLESS_METRICS, refless_data)):\n  fig, axs = plt.subplots(nrows=1, ncols=len(metrics))\n  axs = axs.flatten()\n  for metric, ax in zip(metrics, axs, strict=True):\n    ax.grid(alpha=0.7)\n    ax.tick_params(axis='x', labelrotation=90)\n    ax.set(\n      xlabel='File',\n      # ylabel=PRETTY_METRIC_NAMES[metric],\n      title=PRETTY_METRIC_NAMES[metric]\n    )\n    for series, label in zip(data[metric], PIPELINES, strict=True):\n      ax.plot(\n        FILENAMES_NO_EXTENSION, series,\n        label=label, color=PIPELINE_COLORS[label]\n      )\n\n  handles, labels = axs[0].get_legend_handles_labels()\n  fig.legend(handles, labels, loc='upper center', ncol=len(PIPELINES), bbox_to_anchor=(0.5, 1.1))\n  fig.set_size_inches(10, 4)\n  fig.tight_layout()\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Reference metrics\n\n\n\n\n\n\n\n\n\n\n\n(b) Referenceless metrics\n\n\n\n\n\n\nFigure 2.1: Reference and referenceless Metrics for TCLAHE, DCP, UDCP, and TCLAHE -&gt; UDCP on the DeepBlue dataset. Higher filenames are more turbid. 00gt.png has no turbidity.\n\n\n\n\nSee Figure 4.2 for examples of dehazed images.\n\n\nCode\nfrom pathlib import Path\nimport cv2\n\nBASE_PATH = Path('/home/mantis/Documents/ms/underwater-dehazing-toolkit/data/deepblue')\nFILES = ('02.png', '10.png', '20.png')\n\nfig, axs = plt.subplots(nrows=len(FILES), ncols=len(PIPELINES), sharex=True, sharey=True)\nfor file, axs_row in zip(FILES, axs, strict=True):\n  for pipeline, ax in zip(PIPELINES, axs_row, strict=True):\n    image = cv2.imread(str(BASE_PATH / pipeline / file))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image_shape = image.shape\n    ax.imshow(image)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set(\n      xlabel=pipeline,\n      ylabel=file\n    )\n    ax.label_outer()\nfig.subplots_adjust(wspace=0, hspace=0, left=0, right=1, top=1, bottom=0)\nsubplot_width = 2\nsubplot_height = (subplot_width / image_shape[1] * image_shape[0])\nfig.set_size_inches(\n  subplot_width * 5,\n  subplot_height * 3\n)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2.2: Low (02.png), medium (10.png), and high (20.png) turbidity images from the DeepBlue dataset, processed by TCLAHE, DCP, UDCP, and TCLAHE -&gt; UDCP. The hazy column are the original images.\n\n\n\n\n\nTCLAHE and TCLAHE -&gt; UDCP are consistantly the best performing pipelines (barring their low SSIM scores). DCP seems to modify images the least.\n\n3 Next Steps\n\nMeasure runtimes\nAdd learning pipelines\nChoose which Retinex/CLAHE/etc. model-free pipelines to use",
    "crumbs": [
      "Progress",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic metrics</span>"
    ]
  },
  {
    "objectID": "2025.07.11.html",
    "href": "2025.07.11.html",
    "title": "3  Runtime Trials",
    "section": "",
    "text": "I measured runtimes for DCP, UDCP, CLAHE, and CIE L*a*b* luminance-channel CLAHE. See Figure 3.1.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport json\n\nDATASET_NAMES = (\n  {\n    'key': 'deepblue_prog',\n    'pretty': 'Deepblue'\n  },\n  {\n    'key': 'milk_prog',\n    'pretty': 'Milk'\n  }\n)\n\nPIPELINES = ('clahe', 'lightness_clahe', 'dcp', 'udcp')\nRESOLUTIONS = ('144', '240', '360', '480', '720', '1080', '1440', '2160')\n\nwith open('../data/timing.json', 'r') as runtimes_file:\n  runtimes_json = json.load(runtimes_file)\n\n  for names in DATASET_NAMES:\n    dataset = runtimes_json[names['key']]\n    fig, axs = plt.subplots(nrows=1, ncols=len(PIPELINES))\n    axs = axs.flatten()\n    for pipeline, ax in zip(PIPELINES, axs, strict=True):\n      seriess = [[] for _ in dataset[pipeline][RESOLUTIONS[0]]]\n      for resolution in RESOLUTIONS:\n        for i, time in enumerate(dataset[pipeline][resolution]):\n          seriess[i].append(time // 1000)\n      ax.grid(alpha=0.7)\n      ax.tick_params(axis='x', labelrotation=90)\n      ax.set(\n        title=pipeline,\n        xlabel='Resolution (p)',\n        ylabel='time (ms)'\n      )\n      for series in seriess:\n        ax.plot(RESOLUTIONS, series)\n    fig.set_size_inches(10, 4)\n    fig.suptitle(names['pretty'])\n    fig.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) Deepblue dataset\n\n\n\n\n\n\n\n\n\n\n\n(b) Milk dataset\n\n\n\n\n\n\nFigure 3.1: Timing trials. All images are 16:9; the dimension given is the vertical.\n\n\n\n\n\n4 Next Steps\n\nMeasure penetration improvement for new Erie data\nPossibly investigate speed improvements for DCP/UDCP (already using OpenMP, but seems unexpectedly slow)\nAdd learning pipelines\nChoose which Retinex/CLAHE/etc. model-free pipelines to use\nParameter searching",
    "crumbs": [
      "Progress",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Runtime Trials</span>"
    ]
  },
  {
    "objectID": "2025.07.31.html",
    "href": "2025.07.31.html",
    "title": "4  Presentation Figures",
    "section": "",
    "text": "Figure 4.1 demonstrates the complex operating environment targeted by this project. Shallow water dehazing is unusual in the literature, and its large-particulate, inconsistantly illuminated, high turbidity poses challenges that reduce the effectiveness of current state-of-the-art pipelines.\n\n\n\n\n\n\nFigure 4.1: Operating Environment\n\n\n\nFigure 4.2 displays sample outputs for pipelines representing major areas of dehazing research. UDCP is model-based, (T)CLAHE is model-free, and EDN-GTM uses machine learning. Parameters for CLAHE & TCLAHE were tuned with a manual grid search on a per-image basis. EDN-GTM is trained on an in-air dataset, so its performance could be improved by retraining on underwater data.\n\n\nCode\nimport numpy as np\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\n\nTILE_WIDTH = 1280\nTILE_HEIGHT = 720\n\nOG_IMGS = (\n    \"/home/mantis/Documents/ms/data/cup/0.png\",\n    \"/home/mantis/Documents/ms/data/turbid/deepblue/prog/1080/13.jpg\",\n    \"/home/mantis/Documents/ms/data/turbid/milk/prog/1080/19.jpg\",\n    \"/home/mantis/Documents/ms/data/lakes/erie_0/0.png\",\n    \"/home/mantis/Documents/ms/data/lakes/erie_2025_03/frames/blue_1.png\",\n    \"/home/mantis/Documents/ms/data/lakes/erie_2025_07/frames/png/LT_35_100_frame.png\",\n)\n\nSINGLES_DIR = '/home/mantis/Documents/ms/data/output/summer_end/singles/'\nALGO_DIRS = ('hazy', 'udcp', 'clahe', 'tclahe', 'edn-gtm')\n\nIMAGE_NAMES = ('Cup', 'Deepblue', 'Milk', 'Mr. Crab', 'Erie #1', 'Erie #2')\n\nGRID_WIDTH = len(IMAGE_NAMES)\nGRID_HEIGHT = len(ALGO_DIRS)\n\nfig, axes = plt.subplots(GRID_HEIGHT, GRID_WIDTH)\n\nfor r, dir in enumerate(ALGO_DIRS):\n    if dir == 'hazy':\n        image_files = OG_IMGS\n    else:\n        image_files = sorted([(SINGLES_DIR + dir + '/' + f) for f in os.listdir(SINGLES_DIR + dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n    for c, filepath in enumerate(image_files):\n        img = cv2.imread(filepath, cv2.IMREAD_COLOR_RGB)\n        if img is None:\n            raise FileNotFoundError(f'Could not find {filepath}')\n        img = cv2.resize(img, (TILE_WIDTH, TILE_HEIGHT), interpolation=cv2.INTER_LINEAR)\n\n        ax = axes[r, c]\n        ax.imshow(img)\n\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_xticklabels([])\n        ax.set_yticklabels([])\n        ax.set(\n            xlabel=IMAGE_NAMES[c],\n            ylabel=ALGO_DIRS[r].upper()\n        )\n        ax.xaxis.set_label_position('top')\n        ax.xaxis.tick_top()\n        ax.label_outer()\n\nMARGIN_LR = 0.023\nMARGIN_TB = 0.04\nfig.subplots_adjust(wspace=0, hspace=0,\n                    left=MARGIN_LR, right=1 - MARGIN_LR,\n                    top=1 - MARGIN_TB, bottom=MARGIN_TB)\nsubplot_width = 2\nsubplot_height = (subplot_width / TILE_WIDTH * TILE_HEIGHT)\nfig.set_size_inches(\n  (1 + 2 * MARGIN_LR) * subplot_width * GRID_WIDTH,\n  (1 + 2 * MARGIN_TB) * subplot_height * GRID_HEIGHT\n)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4.2: Reference metrics\n\n\n\n\nSample outputs for UDCP (Underwater Dark Channel Prior; model-based), CLAHE (Contrast-Limit Adaptive Histogram Equalization; model-free), TCLAHE (Turbidity-Aware ibid; model-free), and EDN-GTM (Encoder-Decoder Network with Guided Transmission Map; learning). The first row holds the original hazy images. Images are taken from the underwater cup [1] and turbid (deepblue & milk) [2] datasets, as well as two videos of UXOs in lake Erie.\n\n\n\n\n\n\n\n[1] Y. Zheng, H. Lu, J. Wang, W. Zhang, and M. Guizani, “High-turbidity underwater image enhancement via turbidity suppression fusion,” IEEE Transactions on Circuits and Systems for Video Technology, vol. 35, no. 4, pp. 3527–3540, 2025, doi: 10.1109/TCSVT.2024.3508102.\n\n\n[2] A. Duarte, F. Codevilla, J. D. O. Gaya, and S. S. Botelho, “A dataset to evaluate underwater image restoration methods,” in OCEANS 2016-shanghai, IEEE, 2016, pp. 1–6.",
    "crumbs": [
      "Progress",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Presentation Figures</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "[1] N.\nCarlevaris-Bianco, A. Mohan, and R. M. Eustice, “Initial results\nin underwater single image dehazing,” in OCEANS\n2010 MTS/IEEE SEATTLE, Sep.\n2010, pp. 1–8. doi: 10.1109/OCEANS.2010.5664428.\n\n\n[2] J.\nS. Jaffe, “Computer modeling and the design of optimal underwater\nimaging systems,” IEEE Journal of Oceanic\nEngineering, vol. 15, no. 2, pp. 101–111, Apr. 1990, doi: 10.1109/48.50695.\n\n\n[3] Y.\nY. Schechner and N. Karpel, “Clear underwater vision,” in\nProceedings of the 2004 IEEE computer society\nconference on computer vision and pattern recognition, 2004.\nCVPR 2004., Jun. 2004, pp. I–I. doi: 10.1109/CVPR.2004.1315078.\n\n\n[4] K.\nHe, J. Sun, and X. Tang, “Single image haze removal using dark\nchannel prior,” in 2009 IEEE conference on\ncomputer vision and pattern recognition, Jun. 2009, pp. 1956–1963.\ndoi: 10.1109/CVPR.2009.5206515.\n\n\n[5] P.\nDrews, E. R. Nascimento, F. Moraes, S. S. C. Botelho, and M. F.\nMontenegro Campos, “Transmission estimation in underwater single\nimages,” in 2013 IEEE international conference\non computer vision workshops, Dec. 2013, pp. 825–830. doi: 10.1109/ICCVW.2013.113.\n\n\n[6] A.\nDuarte, F. Codevilla, J. D. O. Gaya, and S. S. Botelho, “A dataset\nto evaluate underwater image restoration methods,” in OCEANS\n2016-shanghai, IEEE, 2016, pp. 1–6.\n\n\n[7] D.\nBerman, T. Treibitz, and S. Avidan, “Non-local image\ndehazing,” in 2016 IEEE conference on computer\nvision and pattern recognition (CVPR), Jun. 2016, pp.\n1674–1682. doi: 10.1109/CVPR.2016.185.\n\n\n[8] D.\nBerman, D. Levy, S. Avidan, and T. Treibitz, “Underwater single\nimage color restoration using haze-lines and a new quantitative\ndataset,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol. 43, no. 8, pp. 2822–2837, Aug. 2021,\ndoi: 10.1109/TPAMI.2020.2977624.\n\n\n[9] C.\nLi et al., “An underwater image enhancement benchmark\ndataset and beyond,” IEEE Transactions on Image\nProcessing, vol. 29, pp. 4376–4389, 2020, doi: 10.1109/TIP.2019.2955241.\n\n\n[10] C.\nLi, S. Anwar, and F. Porikli, “Underwater scene prior inspired\ndeep underwater image and video enhancement,” Pattern\nRecognition, vol. 98, p. 107038, Feb. 2020, doi: 10.1016/j.patcog.2019.107038.\n\n\n[11] Y.\nWang, J. Guo, H. Gao, and H. Yue,\n“UIEC^2-net: CNN-based\nunderwater image enhancement using two color space,” Signal\nProcessing: Image Communication, vol. 96, p. 116250, Aug. 2021,\ndoi: 10.1016/j.image.2021.116250.\n\n\n[12] C.\nQing, F. Yu, X. Xu, W. Huang, and J. Jin, “Underwater video\ndehazing based on spatial–temporal information fusion,”\nMultidim Syst Sign Process, vol. 27, no. 4, pp. 909–924, Oct.\n2016, doi: 10.1007/s11045-016-0407-2.\n\n\n[13] W.\nSong, Y. Wang, D. Huang, and D. Tjondronegoro, “A rapid scene\ndepth estimation model based on underwater light attenuation prior for\nunderwater image restoration,” in Advances in multimedia\ninformation processing – PCM 2018, R. Hong, W.-H.\nCheng, T. Yamasaki, M. Wang, and C.-W. Ngo, Eds., Cham: Springer\nInternational Publishing, 2018, pp. 678–688.\n\n\n[14] Y.\nZheng, H. Lu, J. Wang, W. Zhang, and M. Guizani, “High-turbidity\nunderwater image enhancement via turbidity suppression fusion,”\nIEEE Transactions on Circuits and Systems for Video Technology,\nvol. 35, no. 4, pp. 3527–3540, 2025, doi: 10.1109/TCSVT.2024.3508102.",
    "crumbs": [
      "References"
    ]
  }
]